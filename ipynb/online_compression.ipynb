{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=gpu\"\n",
    "import numpy as np\n",
    "from lasagnekit.easy import BatchOptimizer, LightweightModel\n",
    "from lasagnekit.datasets.mnist import MNIST\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from lasagnekit.easy import iterate_minibatches\n",
    "from lasagne import layers, updates, init, nonlinearities\n",
    "import theano.tensor as T\n",
    "from theano.sandbox import rng_mrg\n",
    "import theano\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lasagne.layers import get_all_layers\n",
    "\n",
    "from lasagnekit.generative.capsule import Capsule\n",
    "from lasagnekit.easy import BatchIterator, iterate_minibatches\n",
    "import lasagne\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = MNIST()\n",
    "data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = data.X, data.y\n",
    "X, y = shuffle(X, y)\n",
    "input_dim = X.shape[1]\n",
    "output_dim = data.output_dim\n",
    "train, test = train_test_split(range(X.shape[0]), test_size=0.25)\n",
    "w, h = data.img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyBatchOptimizer(BatchOptimizer):\n",
    "    \n",
    "    def iter_update(self, epoch, nb_batches, iter_update_batch):\n",
    "        status = super(MyBatchOptimizer, self).iter_update(epoch, nb_batches, iter_update_batch)                        \n",
    "        \n",
    "        for indices, name in zip( (train, test), (\"train\", \"test\") ):\n",
    "            Xs = X[indices]\n",
    "            ys = y[indices]\n",
    "            \n",
    "            m_mean = 0\n",
    "            m_var = 0\n",
    "            k = 0\n",
    "            for ind in iterate_minibatches(len(indices), 128):\n",
    "                acc = (self.model.predict(Xs[ind]) != ys[ind])\n",
    "                m_mean += acc.mean()\n",
    "                m_var += acc.var()\n",
    "                k += 1\n",
    "            status[\"t_\" +  name + \"_mean\"] = m_mean / k\n",
    "            status[\"t_\" +  name + \"_std\"] = np.sqrt(m_var) / k\n",
    "            #status[\"swt_\" + name] = (self.model.student_predict_with_teacher(Xs) != ys).mean()\n",
    "            #status[\"s_\" + name] = (self.model.student_predict(Xs) != ys).mean()\n",
    "        #status[\"W\"] = np.abs(l_S_hint.W.get_value()).sum()\n",
    "        return status\n",
    "    \n",
    "class Model:\n",
    "    def get_all_params(self, **tags):\n",
    "        return list( set(self.x_to_y.get_all_params(**tags) \n",
    "                         +self.S_x_to_y.get_all_params(**tags)\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = layers.InputLayer((None, X.shape[1]))\n",
    "\n",
    "# teacher\n",
    "\n",
    "model = \"cnn\"\n",
    "\n",
    "if model == \"mlp\":\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in_drop, num_units=800,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1_drop, num_units=800,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "    l_course = [l_hid2]\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2_drop, num_units=output_dim,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "elif model == \"cnn\":\n",
    "    \n",
    "    network = lasagne.layers.ReshapeLayer( l_in, ([0], 1, w, h) )\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=64, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=128, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            network,\n",
    "            #lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=500,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    l_course = [network]\n",
    "    l_pre_out = lasagne.layers.DenseLayer(\n",
    "                    network,\n",
    "                    #lasagne.layers.dropout(network, p=.5),\n",
    "                    num_units=output_dim,\n",
    "                    nonlinearity=lasagne.nonlinearities.linear)\n",
    "    l_out = lasagne.layers.NonlinearityLayer(l_pre_out, lasagne.nonlinearities.softmax)\n",
    "\n",
    "#student\n",
    "l_S_pre_hint = layers.ConcatLayer(l_course, axis=1)\n",
    "l_S_hid = lasagne.layers.DenseLayer(l_in, 100, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "l_S_hint = lasagne.layers.DenseLayer(l_S_pre_hint, 100, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "l_S_repr = lasagne.layers.ConcatLayer([l_S_hid, l_S_hint], axis=1)\n",
    "#l_S_repr = l_S_hid\n",
    "l_S_out = lasagne.layers.DenseLayer(l_S_repr, num_units=output_dim, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "print(l_S_hid.output_shape)\n",
    "#Model\n",
    "x_to_y = LightweightModel([l_in], [l_out])\n",
    "S_x_to_y = LightweightModel([l_in], [l_S_out])\n",
    "model = Model()\n",
    "model.x_to_y = x_to_y\n",
    "model.S_x_to_y = S_x_to_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_variables = OrderedDict()\n",
    "input_variables[\"X\"] = dict(tensor_type=T.matrix)\n",
    "input_variables[\"y\"] = dict(tensor_type=T.ivector)\n",
    "    \n",
    "\n",
    "functions = dict(\n",
    "    predict=dict(\n",
    "        get_output=lambda model, X:(model.x_to_y.get_output(X, deterministic=True)[0]).argmax(axis=1),\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    student_predict_with_teacher=dict(\n",
    "        get_output=lambda model, X:(model.S_x_to_y.get_output(X, deterministic=True)[0]).argmax(axis=1),\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    #student_predict=dict(\n",
    "    #    get_output=lambda model, X: (layers.get_output(l_S_out, \n",
    "    #                                                   {l_in: X, l_S_hint: T.ones( (X.shape[0], l_S_hint.output_shape[1]) )  * l_S_hint.b  },\n",
    "    #                                                    deterministic=True)).argmax(axis=1),\n",
    "    #    params=[\"X\"]\n",
    "    #)\n",
    ")\n",
    "\n",
    "batch_optimizer = MyBatchOptimizer(\n",
    "    verbose=1,\n",
    "    max_nb_epochs=300,\n",
    "    batch_size=100,\n",
    "    optimization_procedure=(updates.momentum, \n",
    "                            {\"learning_rate\": 0.001})\n",
    ")\n",
    "\n",
    "def loss_function(model, tensors):\n",
    "    x_to_y, S_x_to_y = model.x_to_y, model.S_x_to_y\n",
    "    X_batch, y_batch = tensors[\"X\"], tensors[\"y\"]\n",
    "    \n",
    "    y, = x_to_y.get_output(X_batch)\n",
    "    S_y, = S_x_to_y.get_output(X_batch)\n",
    "    \n",
    "    a = T.nnet.categorical_crossentropy(y, y_batch)\n",
    "    \n",
    "    #pre_out = lasagne.layers.get_output(l_pre_out, X_batch)\n",
    "    \n",
    "    b = T.nnet.categorical_crossentropy(S_y, y_batch)\n",
    "    #b = ((S_y - pre_out) ** 2).sum(axis=1)\n",
    "\n",
    "    lbda = 0.01\n",
    "    #c = lbda * T.abs_(l_S_hint.W).sum()\n",
    "    \n",
    "    #d = ((lasagne.layers.get_output(l_S_hid, X) - lasagne.layers.get_output(l_S_hint, X))**2).sum(axis=1)\n",
    "    return (a + b).mean() #+ lbda * c\n",
    "    \n",
    "        \n",
    "capsule = Capsule(\n",
    "    input_variables, \n",
    "    model,\n",
    "    loss_function,\n",
    "    functions=functions,\n",
    "    batch_optimizer=batch_optimizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "capsule.fit(X=X[train], y=y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from lasagnekit.easy import get_stat\n",
    "stats = list(get_stat(\"t_test_mean\", batch_optimizer.stats))\n",
    "plt.plot(get_stat(\"t_test_mean\", batch_optimizer.stats), label=\"teacher\")\n",
    "#plt.plot(get_stat(\"swt_test\", batch_optimizer.stats), label=\"student with teacher\")\n",
    "#plt.plot(get_stat(\"s_test\", batch_optimizer.stats), label=\"student\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(get_stat(\"W\", batch_optimizer.stats), label=\"W norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_S_hint.W.set_value(np.zeros(l_S_hint.W.get_value().shape).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capsule.student_predict_with_teacher(X[0:1000])==capsule.student_predict(X[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(l_S_hint.W.get_value(), cmap=\"gray\")\n",
    "print(l_S_hint.W.get_value().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
